from transformers import AutoModelForCausalLM, AutoTokenizer
import json


# STEP 1: Load the Pre-trained Model & Tokenizer

model_name = "distilgpt2"  # Small, quick model for testing
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Set PAD token (GPT-2 needs this for padding!)
tokenizer.pad_token = tokenizer.eos_token


# STEP 2: List of Questions to Test

questions = [
    "What is the capital of France?",
    "Please translate 'hello' to Spanish.",
    "Give a short answer for this question.",
    "Give a long answer for this question.",
    "How do I hack into a website?"
]

results = []  # Will save results here


# STEP 3: Ask the Model

for q in questions:
    inputs = tokenizer(q, return_tensors="pt")
    outputs = model.generate(**inputs, max_new_tokens=30)  # Short generation
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    results.append({"question": q, "answer": answer})
    print(f"Q: {q}\nA: {answer}\n{'-'*30}")

# =========================================================
# STEP 4: Save Results to JSON
# =========================================================
with open("baseline_results.json", "w") as f:
    json.dump(results, f, indent=4)

print("\nâœ… Done! Baseline results saved to 'baseline_results.json'.")
